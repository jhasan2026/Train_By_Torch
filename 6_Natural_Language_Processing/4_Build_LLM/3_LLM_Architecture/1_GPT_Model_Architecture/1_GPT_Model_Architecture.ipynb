{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:27.100976Z",
     "start_time": "2025-05-12T06:49:27.095179Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":1024,\n",
    "    \"embedding_dim\":768,\n",
    "    \"num_of_heads\":12,\n",
    "    \"num_of_layers\":12,\n",
    "    \"drop_rate\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:28.928209Z",
     "start_time": "2025-05-12T06:49:27.100976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "id": "7b2471d0cab886a8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:28.932398Z",
     "start_time": "2025-05-12T06:49:28.928209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FakeTransformer(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    "
   ],
   "id": "e771ef0c1bb43a54",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:28.946726Z",
     "start_time": "2025-05-12T06:49:28.932398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FakeLayerNormalization(nn.Module):\n",
    "    def __init__(self,normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return x\n",
    "    "
   ],
   "id": "fd13246d8e5cceaa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:28.953335Z",
     "start_time": "2025-05-12T06:49:28.946726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'])\n",
    "        self.positional_embedding = nn.Embedding(config['context_length'], config['embedding_dim'])\n",
    "        self.dropout_embedding = nn.Dropout(config['drop_rate'])\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[FakeTransformer(config) for _ in range(config['num_of_layers'])]\n",
    "        )\n",
    "        \n",
    "        self.final_normalization = FakeLayerNormalization(config['embedding_dim'])\n",
    "        \n",
    "        self.output_head = nn.Linear(config['embedding_dim'], config['vocab_size'], bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        batch_size, no_of_token = inputs.shape\n",
    "        token_embedding = self.token_embedding(inputs)\n",
    "        positional_embedding = self.positional_embedding(\n",
    "            torch.arange(no_of_token, device=inputs.device)\n",
    "        )\n",
    "        \n",
    "        x = token_embedding + positional_embedding\n",
    "        x = self.dropout_embedding(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_normalization(x)\n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "    \n",
    "        "
   ],
   "id": "2bc58b971a3a038b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:29.173855Z",
     "start_time": "2025-05-12T06:49:28.953335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "batch = []\n",
    "\n",
    "tx1 = \"Every efforts moves you\"\n",
    "tx2 = \"Everyday hold a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(tx1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(tx2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "batch"
   ],
   "id": "c9fe6d7345f7c070",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6109, 4040, 6100,  345],\n",
       "        [6109,  820, 1745,  257]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:29.178469Z",
     "start_time": "2025-05-12T06:49:29.173855Z"
    }
   },
   "cell_type": "code",
   "source": "batch.shape",
   "id": "f71b1956fd23748f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:29.571502Z",
     "start_time": "2025-05-12T06:49:29.178469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "logits"
   ],
   "id": "169a78b0437e6f6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
       "         [-0.8606,  1.3502, -0.2798,  ..., -0.6376,  1.4690,  1.2530],\n",
       "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
       "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
       "\n",
       "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
       "         [-0.9268, -0.0808, -0.4011,  ..., -0.3565,  0.3229,  0.9507],\n",
       "         [-0.0063,  0.6248, -0.0776,  ...,  0.7499,  0.4245, -1.0798],\n",
       "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T06:49:48.029146Z",
     "start_time": "2025-05-12T06:49:48.016937Z"
    }
   },
   "cell_type": "code",
   "source": "logits.shape",
   "id": "d16e8dd17f1b9ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 50257])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "94be3fdb61559f66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
